# -*- coding: utf-8 -*-
"""Lab QML Tester.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWUr3yu4ri0Lr2yHVLsZ7SGnpeKmb14R

## Installs
"""

#!pip install 'qiskit == 0.32.0'
#!pip install 'pennylane == 0.20.0'
#!pip install 'pylatexenc == 2.10'
#!pip install pennylane-qiskit

import os
import sys
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import pennylane as qml
import plotly.express as px
import matplotlib.pyplot as plt
#from pennylane import numpy as np

from qiskit import IBMQ
from qiskit import QuantumCircuit
from qiskit import QuantumRegister
from qiskit.circuit import Parameter
from qiskit import ClassicalRegister
from qiskit.circuit.measure import measure
from pennylane.templates import RandomLayers

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.datasets import load_iris, make_moons, make_circles, make_blobs, load_wine, load_digits
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score


"""# Data Selection, Seeds & Folders"""

name = 'Digits(5)'
hybrid = sys.argv[1] == 'True'
nn_alpha = float(sys.argv[2]) 
qc_alpha = float(sys.argv[3]) 
data_seed = int(sys.argv[4])
noise_seed = int(sys.argv[5])
layer_seed = int(sys.argv[6])
weight_seed = int(sys.argv[7])

#iris = load_iris() # loads the wine data set from sklearn
#X, Y = iris.data, iris.target   # loads independent, dependent variables from dataset


#wine = load_wine() # loads the wine data set from sklearn
#X, Y = wine.data, wine.target   # loads independent, dependent variables from dataset


digits = load_digits(n_class = 5) # loads the digits from MNIST dataset of Scikit Learn
digits.images = digits.images.reshape((len(digits.images), -1))
X, Y = digits.data, digits.target   # loads independent, dependent variables from dataset


#X, Y = make_moons(n_samples = 300, noise = 0.10, random_state = noise_seed) #noise_seed, rng seed for data noise
#X, Y = make_circles(n_samples = 500, noise = 0.01, random_state = noise_seed) #noise_seed, rng seed for data noise
#X, Y = make_blobs(n_samples = 300, centers = 6, cluster_std = 0.65, n_features = 2, random_state = noise_seed)
#colors = ['maroon', 'navy', 'forestgreen', 'darkmagenta', 'darkorange', 'cyan']
#vectorizer = np.vectorize(lambda x: colors[x % len(colors)])
#fig = plt.scatter(X[:,0], X[:,1], c = vectorizer(Y))
#fig.savefig('2D Data')


input_Length = 64 # <--
hidden_Length = 6
output_Length = 5 # <--

num_circuits = 5 # The number of targets/number of circuits

path = '/home/gauss/GiosResearch/' + name
path = (path + '_Hybrid') if hybrid else (path + '_Quantum')
path = path + '_<nn:' + str(nn_alpha)+'>'+'_<qc:' + str(qc_alpha)+'>'
path = path + ''.join('_'+str(seed) for seed in [noise_seed, layer_seed, data_seed, weight_seed])



# Check whether the specified path exists or not
if not os.path.exists(path):
  print(path)
  os.makedirs(path)

"""# Classic Neural Network

## Network Layers
"""

class FCLayer:
    def __init__(self, input_size, output_size):
        self.name = "FC"
        self.input_size = input_size
        self.output_size = output_size
        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)
        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)

    def forward(self, input):
        self.input = input
        return np.dot(input, self.weights) + self.bias

    def backward(self, output_error, alpha):
        input_error = np.dot(output_error, self.weights.T)
        weights_error = np.dot(self.input.T, output_error)
        
        self.weights -= alpha * weights_error
        self.bias -= alpha * output_error
        return input_error

    def getWeights(self):
        return self.weights

class ActivationLayer:
    def __init__(self, activation, activation_prime):
        self.name = activation.__name__
        self.activation = activation
        self.activation_prime = activation_prime
    
    def forward(self, input):
        self.input = input
        return self.activation(input)
    
    def backward(self, output_error, alpha):
        return output_error * self.activation_prime(self.input)

    def getWeights(self):
        return None

class FlattenLayer:
    def __init__(self, input_size):
        self.name = "Flatten"
        self.input_size = input_size

    def forward(self, input):
        return np.reshape(input, (1, -1))
    
    def backward(self, output_error, alpha):
        return np.reshape(output_error, self.input_size)

    def getWeights(self):
        return None

class SoftmaxLayer:
    def __init__(self, input_size):
        self.name = "Softmax"
        self.input_size = input_size
    
    def forward(self, input):
        self.input = input
        tmp = np.exp(input)
        self.output = tmp / np.sum(tmp)
        return self.output
    
    def backward(self, output_error, alpha):
        input_error = np.zeros(output_error.shape)
        out = np.tile(self.output.T, self.input_size)
        return self.output * np.dot(output_error, np.identity(self.input_size) - out)
    
    def getWeights(self):
        return None

"""## Activation & Cost Functions"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    return np.exp(-x) / (1 + np.exp(-x))**2


def tanh(x):
    return np.tanh(x)

def tanh_prime(x):
    return 1 - np.tanh(x)**2


def relu(x):
    return np.maximum(x, 0)

def relu_prime(x):
    return np.array(x >= 0).astype('int')


def linear(x):
  return x

def linear_prime(x):
  return 1

def mse(y_true, y_pred):
    return np.mean(np.power(y_true - y_pred, 2))

def mse_prime(y_true, y_pred):
    return (y_pred - y_true) / y_pred.size

def sse(y_true, y_pred):
    return 0.5 * np.sum(np.power(y_true - y_pred, 2))

def sse_prime(y_true, y_pred):
    return y_pred - y_true

def hotEncode(target, n_categories):
  oneHot = [0]*n_categories
  oneHot[target] = 1
  return oneHot

"""## Network Design"""

class Classic_Network:
  def __init__(self):
    self.layers = []
    self.output = None
    self.weights = None
    self.cost_func = None
    self.folded = False
    self.layerSizes = []
    self.layerTypes = []
    self.dic = {'Flatten':'blue', 'Softmax':'purple', 'sigmoid':'green', 'tanh':'brown', 'relu':'red', 'linear':'yellow'}


  def standard(self, network, cost_func, folding = False):
    self.folded = folding
    self.layers = network
    self.cost_func = cost_func
    self.layerSizes = []
    self.layerTypes = []
   
    for layer in self.layers:
      if layer.name == "Flatten":
        self.layerSizes.append(layer.input_size[1])
      elif layer.name == "FC":
        self.layerSizes.append(layer.output_size)

    for layer in self.layers:
      if layer.name != "FC":
        self.layerTypes.append(layer.name)
    
    self.weights = self.get_weights()


  def get_plot(self, dim = [.1, .9, .1, .9]): 
    fig = plt.figure(figsize=(10, 10))
    ax = fig.gca()
    #ax.set_visible(not fig.get_visible())
    ax.axis('off');
    
    n_layers = len(self.layerSizes)
    left, right, bottom, top = dim
    verticalSpacing = (top - bottom)/float(max(self.layerSizes))
    horizontalSpacing = (right - left)/float(len(self.layerSizes) - 1)

    def getLayerTop(verticalSpacing, layerSize, top, bottom):
        return verticalSpacing*(layerSize - 1)/2 + (top + bottom)/2

    # Nodes
    for n, layerSize in enumerate(self.layerSizes):
        y = verticalSpacing/4
        c = self.dic[self.layerTypes[n]]
        layerTop =  getLayerTop(verticalSpacing, layerSize, top, bottom)
        for m in range(layerSize):

            x = (n*horizontalSpacing + left, layerTop - m*verticalSpacing)
            
            circle = plt.Circle(x, y, color=c, ec='k', zorder=4)
            ax.add_artist(circle);
    # Edges
    for n, (layerSizeA, layerSizeB) in enumerate(zip(self.layerSizes[:-1], self.layerSizes[1:])):
        layerTopA = getLayerTop(verticalSpacing, layerSizeA, top, bottom)
        layerTopB = getLayerTop(verticalSpacing, layerSizeB, top, bottom) 

        x = [n*horizontalSpacing + left, (n + 1)*horizontalSpacing + left]
        for m in range(layerSizeA):
            originY = layerTopA - m*verticalSpacing
            for o in range(layerSizeB):
                y = [originY, layerTopB - o*verticalSpacing]
                line = plt.Line2D(x, y, c='k')
                ax.add_artist(line);

    
    markers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in self.dic.values()]
    names = [name.capitalize() for name in self.dic.keys()]
    ax.legend(markers, names, prop={'size': 15});
    plt.close()
    return fig


  def get_weights(self):
    weights = []
    for layer in self.layers:
      layerWeights = layer.getWeights()
      layerWeights = list(np.array(layerWeights).flat)
      weights = np.hstack((weights, layerWeights))
    return weights


  def forward_propagation(self, input):
    output = input
    for layer in self.layers:
      output = layer.forward(output)

    self.output = output[0]
    return self.output


  def predict(self, input):
    return np.argmax(self.forward_propagation(input))


  def backward_propagation(self, input, target, alpha = 0.1):

    output = self.forward_propagation(input)
    
    actual = hotEncode(target, len(output))
    output_error = self.cost_func(actual, output)
    
    for layer in reversed(self.layers):
        output_error = layer.backward(output_error, alpha)

    self.weights = self.get_weights()

  def fold(self):
    layers = self.layers[:-2]
    self.standard(layers, self.cost_func, folding = True)

"""# Data Preparation

## Create CNN
"""

np.random.seed(seed = layer_seed)
network = [FlattenLayer(input_size=(1, input_Length)),
          
                   FCLayer(input_Length, hidden_Length),
                   ActivationLayer(linear, linear_prime),
           
                   FCLayer(hidden_Length, output_Length),
                   ActivationLayer(sigmoid, sigmoid_prime)]

cnn = Classic_Network()
cnn.standard(network, sse_prime)
image = cnn.get_plot()
if hybrid: image.savefig(path + '/Unfolded_Classic_Network')

def dim_pool(cnn, features):
  return cnn.forward_propagation(features)

def dim_pool_all(cnn, X):
  pooled = [0]*len(X)

  for i in range(len(X)):
    pooled[i] = dim_pool(cnn, X[i])

  return pooled

"""## Load Data & Pool Dimensions"""

if hybrid:
  X = MinMaxScaler().fit(X).transform(X)

  for i in range(len(X)):
    cnn.backward_propagation(X[i], Y[i], alpha = nn_alpha)
  
  Y_estimate = [cnn.predict(x) for x in X] 
  print_file = open(path + '/CNN_Transfer_Results.txt', 'w')
  print("CNN Transfer Results: ", file = print_file)
  print(confusion_matrix(Y, Y_estimate), file = print_file) 
  print(precision_score(Y, Y_estimate, average='weighted', labels=np.unique(Y_estimate)), file = print_file) 
  print(accuracy_score(Y, Y_estimate), file = print_file)
  print(recall_score(Y, Y_estimate, average='weighted'), file = print_file)
  print(f1_score(Y, Y_estimate, average='weighted'), file = print_file)
  print_file.close() 

  cnn.fold() # Fold network to connect with quantum model

  X = dim_pool_all(cnn, X) # Pass data through cnn model
  image = cnn.get_plot()
  image.savefig(path + '/Folded_Classic_Network')

X = np.pi*MinMaxScaler().fit(X).transform(X) # Normalizes features of the wine data set and rescale with pi

#data_seed, RNG seed for the train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state = data_seed)
# Split the data into a set of training and testing sets

#print(X[0:10]) # Print normalized and rescaled data, notice the fewer dimensions

"""# Quantum Machine Learning Neural Network

## Device & Circuit Design
"""

#IBMQ.save_account('Insert Token Here')

# qml is the Penny Lane Library
dev = qml.device("default.qubit", wires= 2)

#dev = qml.device('qiskit.ibmq', wires=2, backend='ibmq_santiago', shots=5)
#dev = qml.device('qiskit.ibmq', wires=2, backend='ibmq_qasm_simulator', shots=3)

#dev.capabilities()['backend']

num_feat = 6
num_params = 10
@qml.qnode(dev, diff_method="parameter-shift")
def circuit(features, params):
  
  qml.RX(features[0], wires=0)
  qml.RX(features[1], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[0], wires=0)
  qml.RX(params[1], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  qml.RX(features[2], wires=0)
  qml.RX(features[3], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[2], wires=0)
  qml.RX(params[3], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  qml.RX(features[4], wires=0)
  qml.RX(features[5], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[4], wires=0)
  qml.RX(params[5], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  qml.RX(features[0], wires=0)
  qml.RX(features[1], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[6], wires=0)
  qml.RX(params[7], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  qml.RX(features[2], wires=0)
  qml.RX(features[3], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[8], wires=0)
  qml.RX(params[9], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  qml.RX(features[4], wires=0)
  qml.RX(features[5], wires=1)

  qml.broadcast(qml.CZ, wires=[0, 1], pattern="ring")

  qml.RX(params[8], wires=0)
  qml.RX(params[9], wires=1)

  qml.broadcast(qml.CZ, wires=[1, 0], pattern="ring")

  return qml.expval(qml.PauliZ(0))

"""## Circuit Functions"""

# Calculates the gradient of a single parameter 
def parameter_shift_term(features, params, i):
    shifted = params.copy()
    
    shifted[i] += np.pi/2                 # shift parameter i forward by pi/2
    forward = circuit(features, shifted)  # forward evaluation

    shifted[i] -= np.pi                   # shift parameter i backward by pi/2, (pi/2 - pi = -pi/2)
    backward = circuit(features, shifted) # backward evaluation

    return 0.5* (forward - backward) # the difference between the forward shift and backward shift is twice the gradient

# Calculates the gradients for all parameters in a given circuit
def parameter_shift(features, params):
    gradients = np.zeros([len(params)]) # initialize array of zeros

    for i in range(len(params)):
        gradients[i] = parameter_shift_term(features, params, i) # calculate the gradient for each parameter in the circuit

    return gradients # return the gradients of all parameters in a vector

# Adjusts all parameters in a given circuit using the gradient to 'optimize' or 'de-optimize'
def step_function(features, params, alpha= 0.1, beta = None):
  # Each dependent variable will have possible targets that span that variable (possible values of the variable)
  # Each circuit can be 'optimized' to a specific target or 'de-optimized' to a specific target

  # Each parameter is 'optimized' by moving in the positive direction (positive gradient) or 'de-optimized' by...
  # moving in the negative direction (negative gradient)

  if beta is None:
    params += alpha*parameter_shift(features, params)
  else:
    params += (-alpha*beta)*parameter_shift(features, params) #-alpha/3

  return params

"""## Circuit Prints & Displays"""

# Helper/utility Function for printing circuit values
def print_circuit(features, params):
  print("Expectation value:", circuit(features, params))
  print("\n")
  print("Features: ", features)
  print("Parameters: ", params)
  print("\n")
  print(qml.draw(circuit)(features, params))


# Random features and  parameters for conveying step_function
features = np.random.random([num_feat])
parameters = np.random.random([num_params])

print_file = open(path + '/Quantum_Circuit_BluePrint.txt', 'w')
print("Features: ", features, file = print_file)
print("Parameters: ", parameters, file = print_file)
print("\n", file = print_file)
print(qml.draw(circuit)(features, parameters), file = print_file)
print_file.close()

"""
# Graph Qiskit circuit for visual-aid
#get_qiskit_circ(features, parameters).draw(output='mpl', scale=1.5)

# Print values after step function
print("These are the values of the circuit before the step function:")
#get_qiskit_circ(features, params).draw(output='mpl', scale=1.5)
print_circuit(features, parameters)
parameters = step_function(features, parameters)

print("\n")
print("These are the values of the circuit after the step function:")
print_circuit(features, parameters)
#get_qiskit_circ().draw(output='mpl', scale=1.5)
"""

"""## Classifier Design"""

# Helper/utility function for initializing weights at random, can be seeded
def get_weights(num_circuits, num_params, rng_seed):
  weights = [] # Initialize weights

  np.random.seed(seed= rng_seed) # Seed random number generator for study of anomolies
  for i in range(num_circuits):
    params = 2*np.pi*np.random.random([num_params]) # Generate and append random weights
    weights.append(params)

  return weights # Return random weights

def calc_expectations(features, weights, num_circuits):
  expectations = [0]*num_circuits # Initialize zeros for classifier

  for j in range(len(expectations)):
    expectations[j] = (circuit(features, weights[j]) +1)/2 # Get the expectation value from each circuit, one for each target

  return expectations # Return expectations

def classify_expectations(expectations):

  classification = expectations.index(max(expectations)) # Get the index of the largest expectation value, which is taken as the predicted value

  return classification # Return predicted value

# Predicts the classification of a dependent variable by using a circuit for each target, this is the classifier
def predict(features, weights, num_circuits):

  expectations = calc_expectations(features, weights, num_circuits) # Get the expectation value from each circuit, one for each target
  classification = classify_expectations(expectations) # Get the index of the largest expectation value, which is taken as the predicted value
  
  return classification # Return predicted value

# Predict all classifications of a given array of features
def predict_all(weights, X_test, num_circuits):
  Y_estimate = [0]*len(X_test)

  for i in range(len(X_test)):
    Y_estimate[i] = predict(X_test[i], weights, num_circuits) # Predict the classification for every row of features

  return Y_estimate # Return classifications

# Detect and 'optimize'/'de-optimize' the appropriate circuits, optimizing the entire classifier
def optimize_model(features, target, weights, num_circuits, alpha = 0.1):
 
  not_targets = [j for j in range(num_circuits) if j != target] # Determine which indices are not the current target

  weights[target] = step_function(features, weights[target]) # Preform standard gradient function on circuit associated with target

  target_expectation = circuit(features, weights[target])
  expectations = calc_expectations(features, weights, num_circuits)

  sum_expect = sum(expectations)
  beta = (sum_expect  - target_expectation) /sum_expect
  
  for j in not_targets:
    weights[j] = step_function(features, weights[j], alpha = 0.1, beta = beta) # Preform negative gradient function on other circuits

  return weights

# Generate standard classification metrics in a dictionary
def metrics_test(weights, X_test, Y_test, num_circuits, method= 'weighted'):

  Y_estimate = predict_all(weights, X_test, num_circuits) # Estimate dependent variable for every row in the test set

  # The estimated variable and the actual variable are used to calculate/generate metrics for the classifier

  metrics = {}
  metrics['confMatrix'] = confusion_matrix(Y_test, Y_estimate)                                            # Generate confusion matrix
  metrics['precision'] = precision_score(Y_test, Y_estimate, average=method, labels=np.unique(Y_estimate))# Generate percision score
  metrics['accuracy'] = accuracy_score(Y_test, Y_estimate)                                                # Generate accuracy score
  metrics['recall'] = recall_score(Y_test, Y_estimate, average=method)                                    # Generate recall score
  metrics['f1Score'] = f1_score(Y_test, Y_estimate, average=method)                                       # Generate F1 score
  
  return metrics # Return dictionary of metrics

def percent(value):
  return str(round(value*100, 2))+'%'

# Helper/utility function for printing metrics dictionary
def print_metrics_test(epoch, metrics):
  print_file = open(path + '/Metrics_Tickets_'+str(epoch)+'.txt', 'w')
  width = len(metrics['confMatrix'][0])
  index_vec = ['true: ' + str(i) for i in range(width)]
  columns_vec = ['pred: ' + str(i) for i in range(width)]
  print("         At epoch number: "+str(epoch)+"       ", file = print_file)

  print("-------------------------------------", file = print_file)

  print("Precision: ", percent(metrics['precision']), file = print_file)
  print("Accuracy: ", percent(metrics['accuracy']), file = print_file)
  print("Recall: ", percent(metrics['recall']), file = print_file)
  print("F1: ", percent(metrics['f1Score']), file = print_file)

  print("Confusion Matrix: ", file = print_file)
  cmtx = pd.DataFrame(metrics['confMatrix'], 
    index= index_vec, 
    columns=columns_vec)
  print(cmtx, file = print_file)

  print("*************************************", file = print_file)
  print_file.close()

# The three functions below are for storing values in large arrays, will later be recoded to save in a file (json or simple array txt)

# Record the weights of the classifier in an array
def get_weight_record(weights):
  all_weights = np.reshape(weights, len(weights)*len(weights[0]))
  return all_weights

# Record the metrics of the classifier in an array
def get_metrics_record(metrics):
  return [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1Score']]

# Helper/utility function for stacking array
def stack_data(all_data, new_row):
  return np.vstack([all_data, new_row])

# Train the classifier by selecting which circuit to 'optimize' and which circuits to 'de-optimize' also records all weights and metrics in arrays
def train_model(weights, num_circuits, X_train, X_test, Y_train, Y_test, alpha = 0.1, n_epochs = 10, display = True):

  # Get the metrics evaluation for the classifier with random weights, before any training
  metrics = metrics_test(weights, X_test, Y_test, num_circuits)

  print_metrics_test(0, metrics) if display else None # Printing is optional

  # Record the weights and metrics for the class in array form, this and every other recording can be recoded to write to an external file
  expect_data = [0]*num_circuits
  weight_data = get_weight_record(weights)
  metric_data = get_metrics_record(metrics)

  # Train for several epochs, each epoch is going through the training data set once
  for n in range(n_epochs):

    # Iterate through the training data set
    for i in range(len(X_train)):

      # Optimize the classifier
      weights = optimize_model(X_train[i], Y_train[i], weights, num_circuits, alpha = alpha)

      expectations_i = calc_expectations(X_train[i], weights, num_circuits)

      # Record the weights and metrics after each optimization of the classifier for in-depth study
      metrics = metrics_test(weights, X_test, Y_test, num_circuits)
      weight_data = stack_data(weight_data, get_weight_record(weights))
      metric_data = stack_data(metric_data, get_metrics_record(metrics))

      expect_data = stack_data(expect_data, expectations_i)

    print_metrics_test(n+1, metrics) if display else None # Printing is optional

  # Convert the records of the weights and metrics to dataframe form for easier study, can be easily commented out
  w_df = pd.DataFrame(weight_data)
  e_df = pd.DataFrame(expect_data)
  m_df = pd.DataFrame(metric_data, columns=['accuracy', 'percision', 'recall', 'f1Score'])

  return weights, w_df, m_df, e_df # Returns the final 'trained' weights, weight dataframe and metrics dataframe

"""## Training"""

alpha = qc_alpha
n_epochs = 8
#weight_seed, Seed np random number generator
weights = get_weights(num_circuits, num_params, weight_seed)

# Train a data set for use in analysis, set display= False to avoid printing
weights, w_df, m_df, e_df= train_model(weights, num_circuits, X_train, X_test, Y_train, Y_test, alpha = alpha, n_epochs = n_epochs)

ticket_files = [path + '/Metrics_Tickets_'+str(epoch)+'.txt' for epoch in range(n_epochs+1)]

with open(path + '/Metrics_Tickets.txt', "w") as outfile:
  for file_name in ticket_files:
    with open(file_name) as in_file:
      contents = in_file.read()
      outfile.write(contents)
    os.remove(file_name)
    
"""## Training Results"""

# Print weights, weights are unitless
w_df.plot(figsize=(22,10), title= 'Weights recorded over every optimization', grid=True).title.set_size(15)
plt.savefig(path + '/All_Circuit_Weights')

for i in range(num_circuits):
  circ = w_df.iloc[:, (i+0)*num_params : (i+1)*num_params]
  circ.plot(figsize=(22,10), title= "Circuit " + str(i) + " Parameters over Optimization", grid=True).title.set_size(15)
  plt.savefig(path + '/Circuit_' + str(i) +'_Weights')

# Print metrics, metrics are unitless
m_df.plot(figsize=(22,10), title= 'Metrics recorded over every optimization', grid=True).title.set_size(15)
plt.savefig(path + '/Metrics')

e_df.plot(figsize=(22,10), title= 'Expectations recorded over every optimization', grid=True).title.set_size(15)
plt.savefig(path + '/Expectations')

# The four functions below are for calculating basic statistical information in colums of data or dataframes

# Calculate statistics for a one dimensional array of data
def get_stats(df_col, window_frac = 6):
  stats = {}
  stats['mean'] = df_col.mean()
  stats['minimum'] = min(df_col)
  stats['maximum'] = max(df_col)
  stats['variance'] = df_col.var()
  stats['mean windows'] = [0]*window_frac
  stats['variance windows'] = [0]*window_frac

  window = int(len(df_col)*(1/window_frac))

  beg, end = 0, window
  for i in range(window_frac):
    stats['mean windows'][i] = df_col[beg:end].mean()
    stats['variance windows'][i] = df_col[beg:end].var()
    beg = beg + window
    end = end + window
  return stats # Returns a dictionary


# Calculate statistics for every column in a dataframe
def get_all_stats(df):
  i = 0
  all_stats = {}
  for col in df:
    all_stats['Stats_'+str(i)] = get_stats(df[col])
    i = i+1
  return all_stats # Returns a dictionary of dictionaries


# Prints the statistics from a one dimensional array stored in a dictionary
def print_stats(stats):
  print("mean value: ", round(stats['mean'], 4))
  print("variance value: ", round(stats['variance'], 4))
  print("min & max value: ", round(stats['minimum'], 4), round(stats['maximum'], 4))
  print("mean windows (" + str(len(stats['mean windows'])) + '): ', [round(mean, 4) for mean in stats['mean windows']])
  print("variance windows (" + str(len(stats['variance windows'])) +'): ', [round(var, 4) for var in stats['variance windows']])
  

# Print the statistics from a dataframe stored in a dictionary of dictionaries
def print_all_stats(all_stats):
  print("*************************************"*2)
  for col in all_stats:
    print("Column ", "-"+str(col)+"-", " information")
    print_stats(all_stats[col])
    print("*************************************"*2)

#expectation_stats = get_all_stats(e_df)
#print_all_stats(expectation_stats)

frac_window = 20
window = int( len(e_df)*(1/frac_window) )
e_df.rolling(window).mean().plot(figsize=(22,10), title= 'Expectations rolling mean with window of '+str(window), grid=True).title.set_size(15)
plt.savefig(path + '/Expectations_Rolling_Mean')

frac_window = 20
window = int( len(e_df)*(1/frac_window) )
e_df.rolling(window).var().plot(figsize=(22,10), title= 'Expectations rolling variance with window of '+str(window), grid=True).title.set_size(15)
plt.savefig(path + '/Expectations_Rolling_Variance')

w_df.to_csv(path + '/Weights_DataFrame.zip')
m_df.to_csv(path + '/Metrics_DataFrame.zip')
e_df.to_csv(path + '/Expectations_DataFrame.zip')
